Chapter 7<br/>
< Overview >
===============================


### 머신러닝의 학습 방법
- Gradient descent based learning
- Probability theory based learning
- Information theory based learning
- Distance similarity based learning


### Gradient descent based learning
- 실제 값과 학습된 모델 예측치의 오차 최소화
- 모델의 최적 parameter 찾기가 목적


### 예측 함수와 실제 값의 오차 줄이기
- 오차의 합
> 오차는 양수 또는 음수 가능 -> 상쇄될 수 있다.<br/>
> (ŷ⁽¹⁾ - y⁽¹⁾) + (ŷ⁽²⁾ - y⁽²⁾) + (ŷ⁽³⁾ - y⁽³⁾) + (ŷ⁽⁴⁾ - y⁽⁴⁾)

- 제곱의 합으로 변환
> (ŷ⁽¹⁾ - y⁽¹⁾)² + (ŷ⁽²⁾ - y⁽²⁾)² + (ŷ⁽³⁾ - y⁽³⁾)² + (ŷ⁽⁴⁾ - y⁽⁴⁾)²


- Squared Error
<img src="https://github.com/alstn2468/Python_For_Machine_Learning/blob/master/Chapter.7/img/1.png" width="300" height="auto">
<img src="https://github.com/alstn2468/Python_For_Machine_Learning/blob/master/Chapter.7/img/2.png" width="300" height="auto">

- Squared Error를 최소화 할 수 있는 weight값 발견
> 최소 또는 최대의 문제 -> 미분으로 해결하기<br/>
> 찾고자 하는값 : 𝑤₁, 𝑤₀

<img src="https://github.com/alstn2468/Python_For_Machine_Learning/blob/master/Chapter.7/img/3.png" width="300" height="auto">
