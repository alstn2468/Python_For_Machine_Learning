Chapter 7<br/>
< Overfitting and Regularization >
===============================


## Overfitting


### Overfitting
- 학습데이터 과다 최적화<br/>
-> 새로운 데이터의 예측 하락



### Occam's razor
- 보다 적은 수의 논리로 설명이 가능한 경우,<br/>
많은 수의 논리를 세우지 말라



### Bias - Variance tradeoff
- 학습데이터 과다 최적화<br/>
-> 새로운 데이터의 예측 하락

- High bias<br/>
Under Fitting<br/>
원래 모델에 많이 떨어진 상태<br/>
잘못된 데이터만 계속 학습<br/>
-> 잘못된 Weight만 Update

- High variance<br/>
Over Fitting<br/>
모든 데이터에 민감하게 학습<br/>
Error를 고려하지 않는다.<br/>
-> 모든 Weight가 Update

- bias와 variance는 tradeoff 관계



### Train-Test Error

<img src="https://github.com/alstn2468/Python_For_Machine_Learning/blob/master/Chapter.7/img/28.png" width="300" height="auto">



### Overfitting을 극복하기 위한 방법
- 더 많은 데이터를 활용한다.
- Feature의 개수를 줄인다.
- 적절히 Parameter를 선정한다.
- Regularization(정규화)<br/>
<img src="https://github.com/alstn2468/Python_For_Machine_Learning/blob/master/Chapter.7/img/29.png" width="300" height="auto">
