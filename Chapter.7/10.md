Chapter 7<br/>
< Stochastic Gradient Descent >
===============================


## Full-batch Gradient Descent
- GD는 1개의 데이터를 기준으로 미분<br/>
그러나 일반적으로 GD = (full) batch GD라고 가정
- 모든 데이터 셋으로 학습
- 업데이트 감소 -> 계산상 효율적 증가
- 안정적인 Cost 함수 수렴
- 지역 최적화 가능
- 메모리 문제
- 대규모 dataset -> 모델/파라미터 업데이트가 느려진다.


### Stochastic Gradient Descent
- 원래 의미는 dataset에서 random하게 training sample을 뽑은 후<br/>
학습할 때 사용
- Data를 넣기 전에 Shuffle

<img src="https://github.com/alstn2468/Python_For_Machine_Learning/blob/master/Chapter.7/img/23.png" width="400" height="auto">

- 빈번한 업데이트 모델 성능 및 개선 속도 확인 가능
- 일부 문제에 대해 더 빨리 수렴
- 지역 최적화 회피
- 대용량 데이터 시간이 오래걸린다.
- 더 이상 cost가 줄어들지 않는 시점의 발견의 어려움


### Mini-batch SGD
- 한번에 일정량의 데이터를 랜덤하게 뽑아서 학습
- SGD와 Batch GD를 혼합한 기법
- 가장 일반적으로 많이 쓰이는 기법


### Epoch
- 전체 데이터가 training 데이터에 들어갈 때 카운팅
- Full-batch를 n번 실행하면 n epoch


### Batch-size
- 한번에 학습되는 데이터의 개수
- 총 5,120개의 training data에 512 batch-size면<br/>
몇 번 학습을 해야 1 epoch이 되는가?

<img src="https://github.com/alstn2468/Python_For_Machine_Learning/blob/master/Chapter.7/img/24.png" width="400" height="auto">
