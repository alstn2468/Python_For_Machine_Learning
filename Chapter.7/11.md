Chapter 7<br/>
< SGD implementation issues >
===============================

[[실행 코드]](https://github.com/alstn2468/Python_For_Machine_Learning/blob/master/Chapter.7/11.ipynb)

## SGD를 구현할 때 생각해봐 할 일 들


### Mini-Batch SGD

```python
for epoch in range(epoches): # 전체 Epoch이 iteration이 되는 횟수
    X_copy = np.copy(X)
    if is_SGD: # SGD 여부 -> SGD일 경우 Shuffle
        np.random.shuffle(X_copy)
    batch = len(X_copy) # 한번에 처리하는 BATCH SIZE
    for batch_count in range(batch):
        X_batch = np.copy( # BATCH_SIZE 크기 만큼 X_batch 생성
                    X_copy[batch_count * BATCH_SIZE : (batch_count + 1) * BATCH_SIZE])
    print('Number of epoch : %d' % epoch)
```



### Convergence process
- eta : Learning Rate
- epochs : iteration의 횟수

```python
from sklearn.datasets.samples_generator import make_regression
X, y = make_regression(n_samples=1000,
                      n_features=1,
                      noise=10,
                      random_state=42)
```

```python
X[:10]
```



    array([[-1.75873949],
           [ 1.03184454],
           [-0.48760622],
           [ 0.18645431],
           [ 0.72576662],
           [ 0.97255445],
           [ 0.64537595],
           [ 0.68189149],
           [-1.43014138],
           [ 1.06667469]])


```python
plt.plot(X, y, 'o', alpha=0.5)
```

<img src="https://github.com/alstn2468/Python_For_Machine_Learning/blob/master/Chapter.7/img/25.png" width="400" height="auto">



### Time - Consuming
1. Stochastic Gradient Descent
2. Gradient Descent
3. Minibatch-SGD
4. Full-Batch Gradient Descent



### Learning Rate는 일정해야 하는가?
- 한번 내려갈 때마다 조금씩 내려주면 효율적



### Learning-rate decay
- 일정한 주기로 Learning rate를 감소시키는 방법
- 특정 epoch 마다 Learning rate를 감소

```python
self._eta0 = self._eta0 * self._learning_rate_decay
```

- Hyper-parameter 설정의 어려움
- 지수감소<br/>
<img src="https://github.com/alstn2468/Python_For_Machine_Learning/blob/master/Chapter.7/img/26.png" width="100" height="auto">

- 1/t감소<br/>
<img src="https://github.com/alstn2468/Python_For_Machine_Learning/blob/master/Chapter.7/img/27.png" width="100" height="auto">



### 종료조건의 설정
- SGD과정에서 특정 값이하로 cost function 줄어들지 않을 경우<br/>
GD를 멈추는 방법
- 성능이 좋아지지 않는 필요없는 연산을 방지
- 종료조건을 설정 - tol > loss - previous_loss
- tol은 hyperparameter로 설정
