Chapter 7<br/>
< Overview >
===============================


### 머신러닝의 학습 방법
- Gradient descent based learning
- Probability theory based learning
- Information theory based learning
- Distance similarity based learning


### Gradient descent based learning
- 실제 값과 학습된 모델 예측치의 오차 최소화
- 모델의 최적 parameter 찾기가 목적


### 예측 함수와 실제 값의 오차 줄이기
- 오차의 합
> 오차는 양수 또는 음수 가능 -> 상쇄될 수 있다.<br/>
> \left(ŷ⁽¹⁾ - y⁽¹⁾\right) + \left(ŷ⁽²⁾ - y⁽²⁾\right) + \left(ŷ⁽³⁾ - y⁽³⁾\right) + \left(ŷ⁽⁴⁾ - y⁽⁴⁾\right)

- 제곱의 합으로 변환
> \left(ŷ⁽¹⁾ - y⁽¹⁾\right)^2 + \left(ŷ⁽²⁾ - y⁽²⁾\right)^2 + \left(ŷ⁽³⁾ - y⁽³⁾\right)^2 + \left(ŷ⁽⁴⁾ - y⁽⁴⁾\right)^2 


- Squared Error
```
 𝒏                     [  𝑤₁ × 859 + 𝑤₀]        [487]
∑ (ŷ⁽ⁱ⁾ - y⁽ⁱ⁾)²  ŷ =  [𝑤₁ × 10132 + 𝑤₀]   y =  [612]
ⁱ⁼¹                    [𝑤₁ × 12078 + 𝑤₀]        [866]
                       [𝑤₁ × 16430 + 𝑤₀]       [1030]

            [  (𝑤₁ × 859 + 𝑤₀ - 487)² ]
(ŷ - y)² =  [(𝑤₁ × 10132 + 𝑤₀ - 612)² ]
            [(𝑤₁ × 12078 + 𝑤₀ - 866)² ]
            [(𝑤₁ × 16430 + 𝑤₀ - 1030)²]
```

- Squared Error를 최소화 할 수 있는 weight값 발견
> 최소 또는 최대의 문제 -> 미분으로 해결하기<br/>
> 찾고자 하는값 : 𝑤₁, 𝑤₀

```
𝒏                     
𝛴 (𝑤₁x⁽ⁱ⁾ + 𝑤₀ × 1 - y⁽ⁱ⁾)²
ⁱ⁼¹       
```
